save_data: data/wmt16_ro_en/run/
## Where the vocab(s) will be written
src_vocab: data/wmt16_ro_en/run/vocab.src
# tgt_vocab: data/wmt16_ro_en/run/vocab.tgt
overwrite: False

# Corpus opts:
data:
    train:
        path_src: data/wmt16_ro_en/train.en
        path_tgt: data/wmt16_ro_en/train.ro
    valid:
        path_src: data/wmt16_ro_en/valid.en
        path_tgt: data/wmt16_ro_en/valid.ro

# common vocabulary for source and target
share_vocab: True

#### Filter
src_seq_length: 150
tgt_seq_length: 150

# silently ignore empty lines in the data
skip_empty_level: silent

# load from checkpoint
train_from: data/wmt16_ro_en/run/model_step_100000.pt

# maximum vocab size
src_vocab_size: 50000
tgt_vocab_size: 50000

# General opts
keep_checkpoint: 50
save_checkpoint_steps: 5000
report_every: 100
train_steps: 200000
valid_steps: 5000
save_model: data/wmt16_ro_en/run/model
copy_attn: true
global_attention: mlp
word_vec_size: 128
rnn_size: 512
layers: 1
encoder_type: brnn
max_grad_norm: 2
dropout: 0
batch_size: 64
valid_batch_size: 64
optim: adagrad
learning_rate: 0.15
adagrad_accumulator_init: 0.1
reuse_copy_attn: true
copy_loss_by_seqlength: true
bridge: true
seed: 1234
world_size: 1
gpu_ranks: [0]
